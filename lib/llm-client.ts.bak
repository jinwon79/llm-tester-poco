import { GoogleGenerativeAI } from '@google/generative-ai';
import Anthropic from '@anthropic-ai/sdk';
import OpenAI from 'openai';

export type ModelProvider = 'gemini-pro' | 'gemini-3-pro-preview' | 'gemini-flash' | 'claude' | 'gpt';

export interface TestInput {
  testTitle: string;
  systemPromptVersion?: string;
  systemPrompt?: string;
  userMessage: string;
  models: ModelProvider[];
  repeatCount: number;
}

export interface ModelResult {
  model: string;
  repeatIndex: number;
  response: string;
  inputTokens: number;
  outputTokens: number;
  totalTokens: number;
  latencyMs: number;
  error?: string;
}

export interface AnalysisResult {
  targetModel: string;
  judgeModel: string;
  analysis: string;
  score: number;
  verdict: 'Pass' | 'Fail';
}

export interface PerformanceResult {
  test_metadata: { model: string; total_trials: number };
  results: {
    id: number;
    breakdown: { accuracy: number; adherence: number; quality: number };
    total_score: number;
    reason: string;
  }[];
  final_summary: {
    avg_score: number;
    pass_count: number;
    consistency_std_dev: string;
  };
  judgeModel: string;
}

// 클라이언트 초기화
const genAI = new GoogleGenerativeAI(process.env.GOOGLE_GENERATIVE_AI_API_KEY!);
const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export async function runTest(input: TestInput): Promise<ModelResult[]> {
  const results: ModelResult[] = [];

  for (const modelType of input.models) {
    for (let i = 1; i <= input.repeatCount; i++) {
      const startTime = Date.now();
      let result: ModelResult = {
        model: modelType,
        repeatIndex: i,
        response: '',
        inputTokens: 0,
        outputTokens: 0,
        totalTokens: 0,
        latencyMs: 0,
      };

      try {
        if (modelType.startsWith('gemini')) {
          let modelName = '';
          if (modelType === 'gemini-pro') modelName = 'gemini-2.5-pro';
          if (modelType === 'gemini-3-pro-preview') modelName = 'gemini-3-pro-preview';
          if (modelType === 'gemini-flash') modelName = 'gemini-3-flash-preview';

          let model = genAI.getGenerativeModel({ model: modelName });

          if (input.systemPrompt) {
            model = genAI.getGenerativeModel({
              model: modelName,
              systemInstruction: input.systemPrompt
            });
          }

          const resultGen = await model.generateContent(input.userMessage);
          const response = await resultGen.response;
          const text = response.text();
          const usage = response.usageMetadata;

          result.model = modelName;
          result.response = text;
          result.inputTokens = usage?.promptTokenCount || 0;
          result.outputTokens = usage?.candidatesTokenCount || 0;
          result.totalTokens = usage?.totalTokenCount || 0;

        } else if (modelType === 'claude') {
          const modelName = 'claude-sonnet-4-5';
          const msg = await anthropic.messages.create({
            model: modelName,
            max_tokens: 4096,
            system: input.systemPrompt,
            messages: [{ role: 'user', content: input.userMessage }],
          });

          const text = msg.content[0].type === 'text' ? msg.content[0].text : '';

          result.model = 'Claude 4.5 Sonnet';
          result.response = text;
          result.inputTokens = msg.usage.input_tokens;
          result.outputTokens = msg.usage.output_tokens;
          result.totalTokens = msg.usage.input_tokens + msg.usage.output_tokens;

        } else if (modelType === 'gpt') {
          const modelName = 'gpt-4.1';
          const completion = await openai.chat.completions.create({
            messages: [
              { role: 'system', content: input.systemPrompt || '' },
              { role: 'user', content: input.userMessage },
            ],
            model: modelName,
          });

          result.model = 'GPT-4.1';
          result.response = completion.choices[0].message.content || '';
          result.inputTokens = completion.usage?.prompt_tokens || 0;
          result.outputTokens = completion.usage?.completion_tokens || 0;
          result.totalTokens = completion.usage?.total_tokens || 0;
        }

        result.latencyMs = Date.now() - startTime;
        results.push(result);

      } catch (error) {
        result.latencyMs = Date.now() - startTime;
        result.error = error instanceof Error ? error.message : 'Unknown error';
        results.push(result);
      }
    }
  }

  return results;
}

export async function evaluateConsistency(
  targetModel: string,
  referenceResponse: string,
  compareResponses: string[],
  judgeProvider: 'gpt' | 'claude'
): Promise<AnalysisResult> {
  const compareText = compareResponses.map((r, i) => `[비교 답변 ${i + 2}]: ${r}`).join('\n\n');

  const prompt = `### Role: 전문 품질 평가관
### Task: 동일한 질문에 대한 LLM 답변들의 '의미적 일관성' 평가

당신은 AI 서비스의 품질을 평가하는 10년 차 전문 언어 검수사입니다. 
아래의 [기준 답변]과 여러 개의 [비교 답변]들을 대조하여 정보의 평탄도와 논리적 일관성을 판결하십시오.

[기준 답변 (1회차)]: 
${referenceResponse}

${compareText}

### 평가 가이드라인:
1. 정보 완전성 (Information Integrity, 5점):
   - 기준 답변의 핵심 정보가 빠짐없이 포함되었는가?
   - 새로운 내용이 추가되었더라도 기준 답변의 본질을 흐리지 않는가?
2. 형식 및 제약 준수 (Format & Constraint, 3점):
   - 불렛 포인트, 표, JSON 등 요구된 형식이 동일하게 유지되었는가?
   - 답변의 길이(분량)가 유사한 수준인가?
3. 톤앤매너 및 논리 (Tone & Logic, 2점):
   - 말투(종결어미, 전문성 등)가 일관적인가?
   - 기준 답변과 논리적으로 충돌하는 내용(할루시네이션)이 없는가?

### 점수 가이드라인:
- 10점: 두 답변이 문장 구조만 약간 다를 뿐, 의미와 형식이 100% 일치함.
- 8-9점: 핵심 정보는 모두 포함됨. 사소한 표현 차이나 부연 설명이 추가됨.
- 6-7점: 주요 정보는 포함되었으나, 일부 세부 사항이 누락되거나 형식이 미세하게 틀어짐.
- 4-5점: 핵심 정보 중 일부가 누락되었거나, 말투가 기준과 확연히 다름.
- 1-3점: 정보가 왜곡되었거나, 기준 답변과 상충하는 내용이 포함됨. 전혀 다른 형식임.

### 출력 형식 (반드시 아래 형식을 지키고 한국어로 작성할 것):
분석: (답변들 사이의 공통점과 차이점, 일관성 수준을 150자 이내로 요약)
점수: (1~10점 사이의 정수)
판정: (모든 답변이 핵심적으로 일치하면 Pass, 아니면 Fail)
`;

  let responseText = '';
  let judgeModelName = '';

  try {
    if (judgeProvider === 'claude') {
      judgeModelName = 'Claude 4.5 Sonnet';
      const msg = await anthropic.messages.create({
        model: 'claude-sonnet-4-5',
        max_tokens: 2000,
        messages: [{ role: 'user', content: prompt }],
      });
      responseText = msg.content[0].type === 'text' ? msg.content[0].text : '';
    } else {
      judgeModelName = 'GPT-4.1';
      const completion = await openai.chat.completions.create({
        messages: [{ role: 'user', content: prompt }],
        model: 'gpt-4.1',
      });
      responseText = completion.choices[0].message.content || '';
    }

    // 결과 파싱 (마크다운 별표나 공백에 유연하게 대응하도록 정규식 보강)
    const scoreMatch = responseText.match(/점수[:\s*]*(\d+)/);
    const verdictMatch = responseText.match(/판정[:\s*]*(Pass|Fail)/i);
    const analysisMatch = responseText.match(/분석[:\s*]*([\s\S]*?)(?=\n?\s*\*?\*?점수:|$)/);

    return {
      targetModel,
      judgeModel: judgeModelName,
      analysis: analysisMatch ? analysisMatch[1].trim() : responseText.slice(0, 200),
      score: scoreMatch ? parseInt(scoreMatch[1]) : 0,
      verdict: (verdictMatch ? verdictMatch[1].toLowerCase() : 'fail') === 'pass' ? 'Pass' : 'Fail',
    };
  } catch (error) {
    return {
      targetModel,
      judgeModel: judgeProvider === 'claude' ? 'Claude 4.5 Sonnet' : 'GPT-4.1',
      analysis: `평가 중 에러 발생: ${error instanceof Error ? error.message : 'Unknown error'}`,
      score: 0,
      verdict: 'Fail',
    };
  }
}

export async function evaluatePerformance(
  targetModel: string,
  allResponses: string[],
  judgeProvider: 'gpt' | 'claude'
): Promise<PerformanceResult | null> {
  const responsesText = allResponses.map((r, i) => `${i + 1}. ${r}`).join('\n\n');

  const prompt = `### Role: AI 시스템 성능 평가 전문가
### Task: 제시된 ${allResponses.length}개의 모델 답변을 세부 루브릭에 따라 정밀 채점하고 점수를 추출하라.

[대상 모델]: ${targetModel}

[검토할 답변 리스트]:
${responsesText}

### 평가 루브릭 및 가중치:
1. 정확성 (5점): 사실 관계 및 정보 포함 여부
2. 지시 이행 (3점): 형식 및 제약 조건 준수
3. 논리/품질 (2점): 가독성 및 톤앤매너

### 출력 형식 (반드시 JSON만 출력, 코드 블록이나 설명 없이 { }로 시작해서 }로 끝낼 것):
{
  "test_metadata": { "model": "${targetModel}", "total_trials": ${allResponses.length} },
  "results": [
    {
      "id": 1,
      "breakdown": { "accuracy": 5, "adherence": 3, "quality": 2 },
      "total_score": 10,
      "reason": "..."
    }
  ],
  "final_summary": {
    "avg_score": 0.0,
    "pass_count": 0,
    "consistency_std_dev": "..."
  }
}
`;

  let responseText = '';
  let judgeModelName = '';

  try {
    if (judgeProvider === 'claude') {
      judgeModelName = 'Claude 4.5 Sonnet';
      const msg = await anthropic.messages.create({
        model: 'claude-sonnet-4-5',
        max_tokens: 4000,
        messages: [{ role: 'user', content: prompt }],
      });
      responseText = msg.content[0].type === 'text' ? msg.content[0].text : '';
    } else {
      judgeModelName = 'GPT-4.1';
      const completion = await openai.chat.completions.create({
        messages: [{ role: 'user', content: prompt }],
        model: 'gpt-4.1',
        response_format: { type: 'json_object' }
      });
      responseText = completion.choices[0].message.content || '';
    }

    // JSON 추출 (코드 블록 등이 포함될 수 있으므로 정제)
    const jsonStr = responseText.substring(
      responseText.indexOf('{'),
      responseText.lastIndexOf('}') + 1
    );
    const parsed = JSON.parse(jsonStr);

    return {
      ...parsed,
      judgeModel: judgeModelName
    };
  } catch (error) {
    console.error(`Performance evaluation error (${judgeProvider}):`, error);
    return null;
  }
}
